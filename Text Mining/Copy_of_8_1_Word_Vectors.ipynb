{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae40a8a7-eb8f-460b-8d85-8dab164167ac",
      "metadata": {
        "id": "ae40a8a7-eb8f-460b-8d85-8dab164167ac"
      },
      "source": [
        "# 8.1. Introduction to Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104a1fa4-8d45-4470-8e2b-f2b1d3d4fc96",
      "metadata": {
        "id": "104a1fa4-8d45-4470-8e2b-f2b1d3d4fc96"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f7f83f-d778-469b-aae5-123a1529210f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "f6f7f83f-d778-469b-aae5-123a1529210f",
        "outputId": "7410d916-7ae2-45da-da16-6e3aba2ea0a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gensim' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a04dbed129bc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# There are many vectors trained on a variety of corpora with different vector dimensions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# See the documentation of the gensim package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-wiki-gigaword-50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
          ]
        }
      ],
      "source": [
        "# This will download a set of pre-trained word embeddings.\n",
        "# There are many vectors trained on a variety of corpora with different vector dimensions.\n",
        "# See the documentation of the gensim package.\n",
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73a90ac-a8fc-4cb7-98c8-2a8e3ee3926f",
      "metadata": {
        "id": "c73a90ac-a8fc-4cb7-98c8-2a8e3ee3926f"
      },
      "source": [
        "## Additional Resources\n",
        "\n",
        "* [Documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) on the pretrained GLoVe vector models.\n",
        "* [Website](https://nlp.stanford.edu/projects/glove/) for the original GLoVe paper and its pretrained models.\n",
        "* [Book Chapter](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) on Recurrent Neural Networks, which are sequence models that can be used to build classifiers using word embeddings.\n",
        "* [Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) article on embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fabc0f-7a55-4a26-b5f6-429e6faf2292",
      "metadata": {
        "id": "72fabc0f-7a55-4a26-b5f6-429e6faf2292"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "So far in this course, we've done analysis using a *bag of words* model: we assume that every document can be analyzed as a collection of word counts, ignoring their order, and treating each unique token in the document as distinct in meaning. But that's not great for many reasons, including:\n",
        "\n",
        "1. Word *inflections* (\"dog\" vs. \"dogs\") are lost, and we treat the inflected and uninfected words as completely distinct. If we want to take inflection into account, we *lemmatize* the words to their base form and throw away this meaning.\n",
        "2. Words which are synonyms (\"great\" vs. \"excellent\"), or have related meanings (\"king\" vs. \"queen\") similarly are treated as unrelated.\n",
        "3. A single word can have multiple meanings: the verb \"dust\" both means to remove dust (dust the shelves) to apply dust (dust for fingerprints). These are counted as the same word.\n",
        "3. Unrelated homonyms like \"turn\" (to rotate) and \"turn\" (your time to do something) are counted as the same word.\n",
        "4. Word order *does* matter, and it matters quite a lot.\n",
        "\n",
        "An embedding is a way to mitigate these issues, allowing us to relate words to each other and to allow words to have multiple meanings. These won't fix our word order problems, but they are an important step towards doing so with *recurrent* and *convolutional neural networks* (RNNs and CNNs), and understanding *vector embeddings*, which are very broadly useful in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575e4d05-4080-475a-8fcf-e309a8f2d94a",
      "metadata": {
        "id": "575e4d05-4080-475a-8fcf-e309a8f2d94a"
      },
      "source": [
        "#### What is an Embedding?\n",
        "\n",
        "Generally speaking, an *embedding* is a procedure that converts data from a *sparse* format to a *dense* format.\n",
        "\n",
        "**Sparse** data are mostly 0. Word counts are extremely sparse. If we consider a particular document, and list its word counts, assuming a vocabulary size of $N$ (let's say $N = 10,000$), we'll need $N$ numbers to represent the word. A typical word vector will look like this, which is sparse:\n",
        "\n",
        "$$''\\text{aardvark}''\\rightarrow[1, 0, 0, 0, \\cdots]$$\n",
        "$$''\\text{potato}''\\rightarrow[0, 0, \\cdots 0, 1, 0, \\cdots]$$\n",
        "\n",
        "You can think of sparse data as very inefficiently using the storage given to them. Sparse data are like a huge bookshelf with only one or two books on it. What's more, models using sparse data are highly prone to overfitting because they require very complex models: for a vocabulary size of $N$, you need *at least* $N$ parameters.\n",
        "\n",
        "**Dense** data are not full of zeros. Every element of a dense vector is nonzero, for example:\n",
        "\n",
        "$$''\\text{aardvark}''\\rightarrow[2.1, -0.4, 7.2 \\cdots]$$\n",
        "\n",
        "If we can find a high-quality embedding, then words can be described using $D$ numbers, often between 50 and 300 (and $D \\ll N$; often $D \\approx \\sqrt{N}$ works well). This also means that we can make a mode less prone to overfitting, as instead of *at least* $N$ parameters, we can start with only $D$ parameters and achieve the same level of success."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Second Approach\n",
        "\n",
        "What we need is a better way to represent a word's meaning than as a collection of letters. The letters themselves don't convey meaning (and often don't tell you how to pronounce a word, either!). So instead, we'll use numbers.\n",
        "\n",
        "A single number won't work. Why? We might imagine making the number represent how positive or negative a word is. Then \"awesome\" becomes 1, \"good\" is 0.5, \"bad\" is -0.5, \"okay\" is 0, and so on. But there are plenty of other shades of meaning in words beyond how positive or negative they are. Some carry different kinds of feelings. Others represent technical ideas, objects, animals. Anything really.\n",
        "\n",
        "Instead, we'll use a vector, where each word will be represented by multiple numbers. Imagine that each entry in the vector represents some shade of meaning in the word. One represents positivity/negativity, another gender. Whatever. The more entries (the *dimension*) in the vector, the more subtleties we can include for each word. Typical dimensions are 50 to 300. Here's the word \"man\" in one pre-trained set of vectors:"
      ],
      "metadata": {
        "id": "kBOo2E4fYpc3"
      },
      "id": "kBOo2E4fYpc3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce752f62-f8a4-4e90-ba6c-b22a5f5d9a48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce752f62-f8a4-4e90-ba6c-b22a5f5d9a48",
        "outputId": "436c41a5-9355-45c2-af7f-a3196b0d0207"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.094386,  0.43007 , -0.17224 , -0.45529 ,  1.6447  ,  0.40335 ,\n",
              "       -0.37263 ,  0.25071 , -0.10588 ,  0.10778 , -0.10848 ,  0.15181 ,\n",
              "       -0.65396 ,  0.55054 ,  0.59591 , -0.46278 ,  0.11847 ,  0.64448 ,\n",
              "       -0.70948 ,  0.23947 , -0.82905 ,  1.272   ,  0.033021,  0.2935  ,\n",
              "        0.3911  , -2.8094  , -0.70745 ,  0.4106  ,  0.3894  , -0.2913  ,\n",
              "        2.6124  , -0.34576 , -0.16832 ,  0.25154 ,  0.31216 ,  0.31639 ,\n",
              "        0.12539 , -0.012646,  0.22297 , -0.56585 , -0.086264,  0.62549 ,\n",
              "       -0.0576  ,  0.29375 ,  0.66005 , -0.53115 , -0.48233 , -0.97925 ,\n",
              "        0.53135 , -0.11725 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "glove_vectors.get_vector(\"man\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ceb5ac-e94c-4534-9032-7917812a2d15",
      "metadata": {
        "id": "82ceb5ac-e94c-4534-9032-7917812a2d15"
      },
      "source": [
        "It is not immediately clear where a \"gender\" or any other dimension may be, and it might be a combination of several of these.\n",
        "\n",
        "As is often the case in machine learning, we do not tell the algorithm what dimensions we want it to find. It merely finds the ones that fit best. The [the GLoVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf), looks at what words occur in the vicinity of each other. It tries to make the *dot product* of any two word vectors equal to the frequency of their counts. So if a word vector for the $i$th word is written as:\n",
        "\n",
        "$$\\vec{w_i} = \\left<w_1, w_2, w_3, \\cdots w_D\\right>$$\n",
        "\n",
        "Then we want:\n",
        "\n",
        "$$\\vec{w_i}\\cdot\\vec{w_j} + b_i + b_j = w_{i1} w_{j1} + w_{i2} w_{j2} + \\cdots w_{iD}w_{jD} + b_i + b_j = \\ln X_{ij}$$\n",
        "\n",
        "Where $X_{ij}$ is the frequency of words $i$ and $j$ appearing in the same document and $b_i$ and $b_j$ are bias terms (which correspond, roughly, to telling us when a word is very frequent). The bias terms are thrown away after fitting is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "744b3cd7-15c2-4eaf-ac0e-4032415b4f13",
      "metadata": {
        "id": "744b3cd7-15c2-4eaf-ac0e-4032415b4f13"
      },
      "source": [
        "### Uses of Word Vectors\n",
        "\n",
        "##### Distance Between Words\n",
        "\n",
        "If two words are similar in meaning, then they should have similar values in each of the abstract dimensions that make up the word vectors. They should have similar levels of \"positivity\", \"gender\", and so on. This means that they point, roughly, in the same direction.\n",
        "\n",
        "We can ask for *gensim* to calculate the cosine of the angle between two vectors, which gives us a rating from 0 (unrelated) to 1 (very similar). Similarities less than 0 are possible as well, which means the words actively avoid each other in texts.\n",
        "\n",
        "We use the cosine because if the angle between two vectors is small, then $\\cos\\theta \\approx \\cos 0^\\circ = 1$. If they point in very different directions ($90^\\circ$ apart), then $\\cos\\theta \\approx \\cos90^\\circ = 0$. Due to the weirdness of high dimensional spaces, it is quite rare to get vectors that point in opposite directions ($\\cos\\theta < 0$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b012e33-b716-42e6-9ccc-e23cc1c64d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b012e33-b716-42e6-9ccc-e23cc1c64d83",
        "outputId": "18020726-6ece-4e42-968b-1f6e77d82a56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46534905"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "glove_vectors.similarity(\"newyork\", \"binghamton\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf438068-cc82-4b77-aae4-51bfdc1006c3",
      "metadata": {
        "id": "cf438068-cc82-4b77-aae4-51bfdc1006c3"
      },
      "source": [
        "Slightly more quantitatively, we know that if two words show up together quite frequently, then $X_{ij}$ should be quite high. Thus, $\\vec{w}_i\\cdot \\vec{w}_{j}$ should also be relatively large. That means that the cosine of the angle between them:\n",
        "\n",
        "$$\\cos\\theta = \\frac{\\vec{w}_i\\cdot \\vec{w}_{j}}{||\\vec{w}||\\vec{w_j}||}$$\n",
        "\n",
        "Should also be large."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496549ea-8591-4ab8-95f6-13793636acab",
      "metadata": {
        "id": "496549ea-8591-4ab8-95f6-13793636acab"
      },
      "source": [
        "##### Similar Words\n",
        "\n",
        "We can also ask the *gensim* library to tell us what words are most similar to others. It simply calculates the distance to each other word in its vocabulary, and returns the largest ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5b08c8-7dfb-4c9f-a05c-fc3cb5941f38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c5b08c8-7dfb-4c9f-a05c-fc3cb5941f38",
        "outputId": "d30e5d52-55cb-4ae7-a203-13b564f08a7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.8860337734222412),\n",
              " ('boy', 0.8564431071281433),\n",
              " ('another', 0.8452839851379395),\n",
              " ('old', 0.8372183442115784),\n",
              " ('one', 0.827606201171875),\n",
              " ('who', 0.8244695663452148),\n",
              " ('him', 0.8194693922996521),\n",
              " ('turned', 0.8154467940330505),\n",
              " ('whose', 0.811974048614502),\n",
              " ('himself', 0.807725727558136)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "glove_vectors.most_similar('man')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917f54e8-3579-4d3d-a181-b344cdbb7419",
      "metadata": {
        "id": "917f54e8-3579-4d3d-a181-b344cdbb7419"
      },
      "source": [
        "##### Analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178471a5-885b-4de3-b190-cae4f3f8d56a",
      "metadata": {
        "id": "178471a5-885b-4de3-b190-cae4f3f8d56a"
      },
      "source": [
        "As shown on the [GLoVe website](https://nlp.stanford.edu/projects/glove/), the GLoVe algorithm produces linear connections between related concepts. For example, the image below shows the direction and distance between male-female word pairs:\n",
        "\n",
        "<div style=\"text-align:center;\"><img src=\"https://nlp.stanford.edu/projects/glove/images/man_woman.jpg\" width=\"500px\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0c6718-cbfa-4762-9c23-d7d2504b1ee9",
      "metadata": {
        "id": "7e0c6718-cbfa-4762-9c23-d7d2504b1ee9"
      },
      "source": [
        "We can use this to ask our vectors to complete analogies for us, i.e.:\n",
        "\n",
        "<div style=\"text-align:center\">man : king :: woman : ?</div>\n",
        "\n",
        "Or, what word has the same relationship with \"woman\" that \"king\" has with \"man\"? The answer should be \"queen\". This can be done by adding and subtracting vectors. We imagine subtracting \"man\" from \"king\" then adding \"woman\" to what remains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9795dd11-dec9-4561-98ed-8fb55eecdf96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9795dd11-dec9-4561-98ed-8fb55eecdf96",
        "outputId": "976d3964-5601-4337-a6a9-6ed6082579cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('featured', 0.6686994433403015),\n",
              " ('animated', 0.6653846502304077),\n",
              " ('superhero', 0.664042055606842),\n",
              " ('shadows', 0.6478701829910278),\n",
              " ('films', 0.6459299325942993),\n",
              " ('villains', 0.6420229077339172),\n",
              " ('featuring', 0.6413138508796692),\n",
              " ('depicted', 0.636328935623169),\n",
              " ('black-and-white', 0.6362449526786804),\n",
              " ('feature', 0.6313946843147278)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "glove_vectors.most_similar(positive=['best', \"strongest\", 'dark'], negative=['good', \"strong\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2d8ce6-bc70-4ede-9ac2-83e658dfa9ed",
      "metadata": {
        "id": "4f2d8ce6-bc70-4ede-9ac2-83e658dfa9ed"
      },
      "source": [
        "##### Machine Learning\n",
        "\n",
        "Instead of using word counts, we can use the word vectors and the $x$ variable when we do machine learning. This can be done by averaging the word vectors in a document together, optionally removing stop words."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}